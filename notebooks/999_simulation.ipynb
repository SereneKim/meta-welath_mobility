{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c1fba2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed rank changes: [1. 1. 2. 0.]\n",
      "Mean change: 1.0\n",
      "Std change: 0.7071067811865476\n",
      "\n",
      "E's imputed rank in 2001-2005:\n",
      "Posterior mean: 2.06\n",
      "95% CI: [1, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "test = pd.DataFrame({\n",
    "    'period': ['2001-2005', '2006-2010'] * 5,\n",
    "    'node': ['A', 'B', 'C', 'D', 'E'] * 2,\n",
    "    'rank': [1, 2, 3, 4, None, 2, 1, 5, 4, 3]\n",
    "})\n",
    "\n",
    "# Method 2: Learn from observed rank changes\n",
    "def impute_from_transitions(test, n_samples=1000):\n",
    "    \"\"\"\n",
    "    Observe: A,B,C,D all stayed the same rank\n",
    "    This suggests rank stability\n",
    "    E is rank 5 in period 2, likely rank 5 in period 1\n",
    "    \"\"\"\n",
    "    # Get matched observations\n",
    "    matched = test.dropna()\n",
    "    # Only keep nodes present in both periods\n",
    "    nodes_p1 = set(matched[matched['period'] == '2001-2005']['node'])\n",
    "    nodes_p2 = set(matched[matched['period'] == '2006-2010']['node'])\n",
    "    common_nodes = nodes_p1 & nodes_p2\n",
    "\n",
    "    p1 = matched[(matched['period'] == '2001-2005') & (matched['node'].isin(common_nodes))].sort_values('node')\n",
    "    p2 = matched[(matched['period'] == '2006-2010') & (matched['node'].isin(common_nodes))].sort_values('node')\n",
    "    \n",
    "    # Calculate rank changes\n",
    "    rank_changes = p2['rank'].values - p1['rank'].values\n",
    "    \n",
    "    print(\"Observed rank changes:\", rank_changes)\n",
    "    print(\"Mean change:\", rank_changes.mean())\n",
    "    print(\"Std change:\", rank_changes.std())\n",
    "    \n",
    "    # E is rank 5 in period 2\n",
    "    # If mean change is ~0, E was likely rank 5 in period 1\n",
    "    # Sample from Normal(5 - mean_change, std_change)\n",
    "    \n",
    "    e_rank_period2 = test[(test['node'] == 'E') & (test['period'] == '2006-2010')]['rank'].values[0]\n",
    "    predicted_rank_p1 = e_rank_period2 - rank_changes.mean()\n",
    "    \n",
    "    # Generate samples with uncertainty\n",
    "    samples = np.random.normal(\n",
    "        loc=predicted_rank_p1,\n",
    "        scale=max(rank_changes.std(), 0.5),  # min std for uncertainty\n",
    "        size=n_samples\n",
    "    )\n",
    "    \n",
    "    # Constrain to valid ranks [1, 5]\n",
    "    samples = np.clip(np.round(samples), 1, 5)\n",
    "    \n",
    "    return samples, rank_changes\n",
    "\n",
    "samples, changes = impute_from_transitions(test)\n",
    "print(f\"\\nE's imputed rank in 2001-2005:\")\n",
    "print(f\"Posterior mean: {samples.mean():.2f}\")\n",
    "print(f\"95% CI: [{np.percentile(samples, 2.5):.0f}, \"\n",
    "      f\"{np.percentile(samples, 97.5):.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3750d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior for rank change distribution:\n",
      "  Mean: 0.941\n",
      "  Std: 0.485\n",
      "\n",
      "E's imputed rank in 2001-2005:\n",
      "Posterior mean: 2.06\n",
      "Posterior median: 2\n",
      "95% Credible Interval: [1.1, 3.0]\n",
      "P(E was rank 2 in period 1) = 0.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Method 3: Full Bayesian Model\n",
    "# Model: rank_t ~ rank_{t-1} + change\n",
    "# where change ~ Normal(mu_change, sigma_change)\n",
    "\n",
    "def bayesian_imputation_mcmc(test, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Bayesian model:\n",
    "    - Prior on rank changes: Normal(0, σ) \n",
    "    - Observe 4 rank changes (all 0)\n",
    "    - Update beliefs about rank stability\n",
    "    - Impute E's period 1 rank given E's period 2 rank = 5\n",
    "    \"\"\"\n",
    "    \n",
    "    # Observed data\n",
    "    matched = test.dropna()\n",
    "    # Only keep nodes present in both periods\n",
    "    nodes_p1 = set(matched[matched['period'] == '2001-2005']['node'])\n",
    "    nodes_p2 = set(matched[matched['period'] == '2006-2010']['node'])\n",
    "    common_nodes = nodes_p1 & nodes_p2\n",
    "\n",
    "    p1 = matched[(matched['period'] == '2001-2005') & (matched['node'].isin(common_nodes))].sort_values('node')\n",
    "    p2 = matched[(matched['period'] == '2006-2010') & (matched['node'].isin(common_nodes))].sort_values('node')\n",
    "    observed_changes = p2['rank'].values - p1['rank'].values\n",
    "    \n",
    "    # Priors\n",
    "    mu_change_prior = 0  # Expect no change on average\n",
    "    sigma_change_prior = 2  # But allow for movement\n",
    "    \n",
    "    # Posterior for change distribution (conjugate normal)\n",
    "    n_obs = len(observed_changes)\n",
    "    \n",
    "    # Update posterior for mean change\n",
    "    posterior_precision = 1/sigma_change_prior**2 + n_obs/1\n",
    "    posterior_mean = (mu_change_prior/sigma_change_prior**2 + \n",
    "                     observed_changes.sum()/1) / posterior_precision\n",
    "    posterior_std = np.sqrt(1/posterior_precision)\n",
    "    \n",
    "    print(f\"Posterior for rank change distribution:\")\n",
    "    print(f\"  Mean: {posterior_mean:.3f}\")\n",
    "    print(f\"  Std: {posterior_std:.3f}\")\n",
    "    \n",
    "    # Now impute E's period 1 rank\n",
    "    # E_rank_p2 = E_rank_p1 + change\n",
    "    # E_rank_p1 = E_rank_p2 - change\n",
    "    \n",
    "    e_rank_p2 = test[(test['node'] == 'E') & (test['period'] == '2006-2010')]['rank'].values[0]\n",
    "    \n",
    "    # Sample changes from posterior\n",
    "    change_samples = np.random.normal(\n",
    "        posterior_mean, \n",
    "        posterior_std, \n",
    "        n_samples\n",
    "    )\n",
    "    \n",
    "    # Impute period 1 ranks\n",
    "    e_rank_p1_samples = e_rank_p2 - change_samples\n",
    "    \n",
    "    # Constrain to valid range\n",
    "    e_rank_p1_samples = np.clip(e_rank_p1_samples, 1, 5)\n",
    "    \n",
    "    return e_rank_p1_samples, posterior_mean, posterior_std\n",
    "\n",
    "samples, post_mu, post_sigma = bayesian_imputation_mcmc(test)\n",
    "\n",
    "print(f\"\\nE's imputed rank in 2001-2005:\")\n",
    "print(f\"Posterior mean: {samples.mean():.2f}\")\n",
    "print(f\"Posterior median: {np.median(samples):.0f}\")\n",
    "print(f\"95% Credible Interval: [{np.percentile(samples, 2.5):.1f}, \"\n",
    "      f\"{np.percentile(samples, 97.5):.1f}]\")\n",
    "\n",
    "# Probability E was rank 2\n",
    "prob_rank_2 = (samples == 2).mean()\n",
    "print(f\"P(E was rank 2 in period 1) = {prob_rank_2:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dfdcf5",
   "metadata": {},
   "source": [
    "# With Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f766952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting period 1 distribution...\n",
      "  P(zero) = 0.25\n",
      "  Non-zero: log-normal(μ=4.90, σ=0.94)\n",
      "\n",
      "Fitting period 2 distribution...\n",
      "  P(zero) = 0.20\n",
      "  Non-zero: log-normal(μ=4.99, σ=1.03)\n",
      "\n",
      "Score transition model (log scale):\n",
      "  log(score_p1) = 0.800 * log(score_p2) + 0.990\n",
      "  Residual std: 0.148\n",
      "  R²: 0.975\n",
      "\n",
      "Node E:\n",
      "  Observed P2 score: 200.0\n",
      "  Imputed P1 score: 191.2 [119.4, 293.5]\n",
      "\n",
      "Node E - Rank distribution in period 1:\n",
      "  Mean rank: 2.0\n",
      "  Median rank: 2\n",
      "  95% CI: [2, 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Your data structure\n",
    "data = pd.DataFrame({\n",
    "    'period': ['2001-2005', '2006-2010'] * 5,\n",
    "    'node': ['A', 'B', 'C', 'D', 'E'] * 2,\n",
    "    'score': [120, 0, 450, 30, None, 150, 0, 520, 45, 200]  # Example\n",
    "})\n",
    "\n",
    "def fit_zero_inflated_model(scores):\n",
    "    \"\"\"\n",
    "    Fit zero-inflated log-normal to observed scores\n",
    "    Returns: (prob_zero, mu_log, sigma_log) for non-zero part\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)\n",
    "    scores = scores[~np.isnan(scores)]\n",
    "    \n",
    "    # Estimate probability of zero\n",
    "    p_zero = (scores == 0).mean()\n",
    "    \n",
    "    # Fit log-normal to non-zero scores\n",
    "    nonzero = scores[scores > 0]\n",
    "    if len(nonzero) > 0:\n",
    "        mu_log = np.log(nonzero).mean()\n",
    "        sigma_log = np.log(nonzero).std()\n",
    "    else:\n",
    "        mu_log, sigma_log = 0, 1\n",
    "    \n",
    "    return p_zero, mu_log, sigma_log\n",
    "\n",
    "def bayesian_score_imputation(data, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Full Bayesian imputation using score information\n",
    "    \"\"\"\n",
    "    # Separate periods\n",
    "    p1 = data[data['period'] == '2001-2005'].set_index('node')\n",
    "    p2 = data[data['period'] == '2006-2010'].set_index('node')\n",
    "    \n",
    "    # Get matched observations\n",
    "    matched_nodes = p1.index.intersection(p2.index)\n",
    "    matched_p1 = p1.loc[matched_nodes, 'score'].dropna()\n",
    "    matched_p2 = p2.loc[matched_nodes, 'score'].dropna()\n",
    "    \n",
    "    # Fit distributions for each period\n",
    "    print(\"Fitting period 1 distribution...\")\n",
    "    p1_params = fit_zero_inflated_model(p1['score'])\n",
    "    print(f\"  P(zero) = {p1_params[0]:.2f}\")\n",
    "    print(f\"  Non-zero: log-normal(μ={p1_params[1]:.2f}, σ={p1_params[2]:.2f})\")\n",
    "    \n",
    "    print(\"\\nFitting period 2 distribution...\")\n",
    "    p2_params = fit_zero_inflated_model(p2['score'])\n",
    "    print(f\"  P(zero) = {p2_params[0]:.2f}\")\n",
    "    print(f\"  Non-zero: log-normal(μ={p2_params[1]:.2f}, σ={p2_params[2]:.2f})\")\n",
    "    \n",
    "    # Model score transition for matched entities\n",
    "    # Simple approach: score_p1 = alpha * score_p2 + noise\n",
    "    # (accounts for scale difference)\n",
    "    \n",
    "    valid_matches = matched_p1.index.intersection(matched_p2.index)\n",
    "    if len(valid_matches) > 1:\n",
    "        s1 = matched_p1[valid_matches].values\n",
    "        s2 = matched_p2[valid_matches].values\n",
    "        \n",
    "        # Filter to non-zero pairs for regression\n",
    "        nonzero_mask = (s1 > 0) & (s2 > 0)\n",
    "        if nonzero_mask.sum() > 1:\n",
    "            # Linear regression in log space\n",
    "            log_s1 = np.log(s1[nonzero_mask] + 1)\n",
    "            log_s2 = np.log(s2[nonzero_mask] + 1)\n",
    "            \n",
    "            slope, intercept, r_val, _, _ = stats.linregress(log_s2, log_s1)\n",
    "            residual_std = np.std(log_s1 - (slope * log_s2 + intercept))\n",
    "            \n",
    "            print(f\"\\nScore transition model (log scale):\")\n",
    "            print(f\"  log(score_p1) = {slope:.3f} * log(score_p2) + {intercept:.3f}\")\n",
    "            print(f\"  Residual std: {residual_std:.3f}\")\n",
    "            print(f\"  R²: {r_val**2:.3f}\")\n",
    "        else:\n",
    "            # Fallback: assume proportional scaling\n",
    "            slope = 500 / 600  # Scale ratio\n",
    "            intercept = 0\n",
    "            residual_std = 1.0\n",
    "    \n",
    "    # Impute missing scores\n",
    "    results = {}\n",
    "    \n",
    "    for node in data['node'].unique():\n",
    "        if pd.isna(p1.loc[node, 'score']) and not pd.isna(p2.loc[node, 'score']):\n",
    "            # Node missing in period 1, present in period 2\n",
    "            score_p2 = p2.loc[node, 'score']\n",
    "            \n",
    "            if score_p2 == 0:\n",
    "                # If zero in P2, likely zero in P1 too\n",
    "                # Sample from Bernoulli then from non-zero dist\n",
    "                prob_was_zero = p1_params[0] / (p1_params[0] + (1-p1_params[0]) * 0.3)\n",
    "                samples = []\n",
    "                for _ in range(n_samples):\n",
    "                    if np.random.rand() < prob_was_zero:\n",
    "                        samples.append(0)\n",
    "                    else:\n",
    "                        samples.append(np.exp(np.random.normal(\n",
    "                            p1_params[1], p1_params[2]\n",
    "                        )))\n",
    "                samples = np.array(samples)\n",
    "            else:\n",
    "                # Non-zero in P2: use transition model\n",
    "                log_s2 = np.log(score_p2 + 1)\n",
    "                predicted_log_s1 = slope * log_s2 + intercept\n",
    "                \n",
    "                # Sample with uncertainty\n",
    "                log_s1_samples = np.random.normal(\n",
    "                    predicted_log_s1,\n",
    "                    residual_std * 1.5,  # Inflate for missing data\n",
    "                    n_samples\n",
    "                )\n",
    "                samples = np.exp(log_s1_samples) - 1\n",
    "                samples = np.clip(samples, 0, 500)  # Respect period 1 range\n",
    "            \n",
    "            results[node] = {\n",
    "                'period_2_score': score_p2,\n",
    "                'imputed_samples': samples,\n",
    "                'mean': samples.mean(),\n",
    "                'median': np.median(samples),\n",
    "                'ci_low': np.percentile(samples, 2.5),\n",
    "                'ci_high': np.percentile(samples, 97.5)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run imputation\n",
    "results = bayesian_score_imputation(data)\n",
    "\n",
    "# Display results\n",
    "for node, result in results.items():\n",
    "    print(f\"\\nNode {node}:\")\n",
    "    print(f\"  Observed P2 score: {result['period_2_score']:.1f}\")\n",
    "    print(f\"  Imputed P1 score: {result['mean']:.1f} \"\n",
    "          f\"[{result['ci_low']:.1f}, {result['ci_high']:.1f}]\")\n",
    "\n",
    "# Convert to ranks (for each sample)\n",
    "def samples_to_rank_distribution(data, results, n_samples=5000):\n",
    "    \"\"\"Convert score samples to rank distributions\"\"\"\n",
    "    p1 = data[data['period'] == '2001-2005'].set_index('node')\n",
    "    \n",
    "    rank_samples = {node: [] for node in results.keys()}\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        # Get scores for this sample\n",
    "        scores = {}\n",
    "        for node in data['node'].unique():\n",
    "            if not pd.isna(p1.loc[node, 'score']):\n",
    "                scores[node] = p1.loc[node, 'score']\n",
    "            elif node in results:\n",
    "                scores[node] = results[node]['imputed_samples'][sample_idx]\n",
    "        \n",
    "        # Rank them (higher score = better rank)\n",
    "        sorted_nodes = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranks = {node: rank+1 for rank, (node, score) in enumerate(sorted_nodes)}\n",
    "        \n",
    "        for node in results.keys():\n",
    "            rank_samples[node].append(ranks[node])\n",
    "    \n",
    "    return rank_samples\n",
    "\n",
    "rank_distributions = samples_to_rank_distribution(data, results)\n",
    "\n",
    "for node, ranks in rank_distributions.items():\n",
    "    print(f\"\\nNode {node} - Rank distribution in period 1:\")\n",
    "    print(f\"  Mean rank: {np.mean(ranks):.1f}\")\n",
    "    print(f\"  Median rank: {np.median(ranks):.0f}\")\n",
    "    print(f\"  95% CI: [{np.percentile(ranks, 2.5):.0f}, \"\n",
    "          f\"{np.percentile(ranks, 97.5):.0f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b761b8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting period 1 distribution...\n",
      "  P(zero) = 0.25\n",
      "  Non-zero: log-normal(μ=4.90, σ=0.94)\n",
      "\n",
      "Fitting period 2 distribution...\n",
      "  P(zero) = 0.20\n",
      "  Non-zero: log-normal(μ=4.99, σ=1.03)\n",
      "\n",
      "Score transition model (log scale):\n",
      "  log(score_p1) = 0.800 * log(score_p2) + 0.990\n",
      "  Residual std: 0.148\n",
      "  R²: 0.975\n",
      "\n",
      "Node E:\n",
      "  Observed P2 score: 200.0\n",
      "  Imputed P1 score: 191.5 [121.3, 285.6]\n",
      "\n",
      "Node E - Rank distribution in period 1:\n",
      "  Mean rank: 2.0\n",
      "  Median rank: 2\n",
      "  95% CI: [2, 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Your data structure\n",
    "data = pd.DataFrame({\n",
    "    'period': ['2001-2005', '2006-2010'] * 5,\n",
    "    'node': ['A', 'B', 'C', 'D', 'E'] * 2,\n",
    "    'score': [120, 0, 450, 30, None, 150, 0, 520, 45, 200]  # Example\n",
    "})\n",
    "\n",
    "def fit_zero_inflated_model(scores):\n",
    "    \"\"\"\n",
    "    Fit zero-inflated log-normal to observed scores\n",
    "    Returns: (prob_zero, mu_log, sigma_log) for non-zero part\n",
    "    \"\"\"\n",
    "    scores = np.array(scores)\n",
    "    scores = scores[~np.isnan(scores)]\n",
    "    \n",
    "    # Estimate probability of zero\n",
    "    p_zero = (scores == 0).mean()\n",
    "    \n",
    "    # Fit log-normal to non-zero scores\n",
    "    nonzero = scores[scores > 0]\n",
    "    if len(nonzero) > 0:\n",
    "        mu_log = np.log(nonzero).mean()\n",
    "        sigma_log = np.log(nonzero).std()\n",
    "    else:\n",
    "        mu_log, sigma_log = 0, 1\n",
    "    \n",
    "    return p_zero, mu_log, sigma_log\n",
    "\n",
    "def bayesian_score_imputation(data, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Full Bayesian imputation using score information\n",
    "    \"\"\"\n",
    "    # Separate periods\n",
    "    p1 = data[data['period'] == '2001-2005'].set_index('node')\n",
    "    p2 = data[data['period'] == '2006-2010'].set_index('node')\n",
    "    \n",
    "    # Get matched observations\n",
    "    matched_nodes = p1.index.intersection(p2.index)\n",
    "    matched_p1 = p1.loc[matched_nodes, 'score'].dropna()\n",
    "    matched_p2 = p2.loc[matched_nodes, 'score'].dropna()\n",
    "    \n",
    "    # Fit distributions for each period\n",
    "    print(\"Fitting period 1 distribution...\")\n",
    "    p1_params = fit_zero_inflated_model(p1['score'])\n",
    "    print(f\"  P(zero) = {p1_params[0]:.2f}\")\n",
    "    print(f\"  Non-zero: log-normal(μ={p1_params[1]:.2f}, σ={p1_params[2]:.2f})\")\n",
    "    \n",
    "    print(\"\\nFitting period 2 distribution...\")\n",
    "    p2_params = fit_zero_inflated_model(p2['score'])\n",
    "    print(f\"  P(zero) = {p2_params[0]:.2f}\")\n",
    "    print(f\"  Non-zero: log-normal(μ={p2_params[1]:.2f}, σ={p2_params[2]:.2f})\")\n",
    "    \n",
    "    # Model score transition for matched entities\n",
    "    # Simple approach: score_p1 = alpha * score_p2 + noise\n",
    "    # (accounts for scale difference)\n",
    "    \n",
    "    valid_matches = matched_p1.index.intersection(matched_p2.index)\n",
    "    if len(valid_matches) > 1:\n",
    "        s1 = matched_p1[valid_matches].values\n",
    "        s2 = matched_p2[valid_matches].values\n",
    "        \n",
    "        # Filter to non-zero pairs for regression\n",
    "        nonzero_mask = (s1 > 0) & (s2 > 0)\n",
    "        if nonzero_mask.sum() > 1:\n",
    "            # Linear regression in log space\n",
    "            log_s1 = np.log(s1[nonzero_mask] + 1)\n",
    "            log_s2 = np.log(s2[nonzero_mask] + 1)\n",
    "            \n",
    "            slope, intercept, r_val, _, _ = stats.linregress(log_s2, log_s1)\n",
    "            residual_std = np.std(log_s1 - (slope * log_s2 + intercept))\n",
    "            \n",
    "            print(f\"\\nScore transition model (log scale):\")\n",
    "            print(f\"  log(score_p1) = {slope:.3f} * log(score_p2) + {intercept:.3f}\")\n",
    "            print(f\"  Residual std: {residual_std:.3f}\")\n",
    "            print(f\"  R²: {r_val**2:.3f}\")\n",
    "        else:\n",
    "            # Fallback: assume proportional scaling\n",
    "            slope = 500 / 600  # Scale ratio\n",
    "            intercept = 0\n",
    "            residual_std = 1.0\n",
    "    \n",
    "    # Impute missing scores\n",
    "    results = {}\n",
    "    \n",
    "    for node in data['node'].unique():\n",
    "        if pd.isna(p1.loc[node, 'score']) and not pd.isna(p2.loc[node, 'score']):\n",
    "            # Node missing in period 1, present in period 2\n",
    "            score_p2 = p2.loc[node, 'score']\n",
    "            \n",
    "            if score_p2 == 0:\n",
    "                # If zero in P2, use Bayes' theorem for P(was zero in P1)\n",
    "                # Estimate transition probabilities from matched entities\n",
    "                \n",
    "                # Count transitions in matched data\n",
    "                zero_to_zero = ((s1 == 0) & (s2 == 0)).sum()\n",
    "                nonzero_to_zero = ((s1 > 0) & (s2 == 0)).sum()\n",
    "                zero_to_nonzero = ((s1 == 0) & (s2 > 0)).sum()\n",
    "                \n",
    "                # Estimate P(P2=0 | P1=0) and P(P2=0 | P1>0)\n",
    "                if (zero_to_zero + zero_to_nonzero) > 0:\n",
    "                    p_stay_zero = zero_to_zero / (zero_to_zero + zero_to_nonzero)\n",
    "                else:\n",
    "                    p_stay_zero = 0.9  # Default: assume high persistence\n",
    "                \n",
    "                if (nonzero_to_zero + (s1 > 0).sum() - nonzero_to_zero) > 0:\n",
    "                    p_drop_to_zero = nonzero_to_zero / (s1 > 0).sum()\n",
    "                else:\n",
    "                    p_drop_to_zero = 0.1  # Default: assume low dropout\n",
    "                \n",
    "                # Bayes' theorem\n",
    "                prior_zero = p1_params[0]\n",
    "                likelihood_zero = p_stay_zero\n",
    "                likelihood_nonzero = p_drop_to_zero\n",
    "                \n",
    "                prob_was_zero = (likelihood_zero * prior_zero) / (\n",
    "                    likelihood_zero * prior_zero + \n",
    "                    likelihood_nonzero * (1 - prior_zero)\n",
    "                )\n",
    "                \n",
    "                print(f\"  Zero in P2 transition probs:\")\n",
    "                print(f\"    P(stay at 0) = {p_stay_zero:.2f}\")\n",
    "                print(f\"    P(drop to 0) = {p_drop_to_zero:.2f}\")\n",
    "                print(f\"    → P(was 0 in P1 | is 0 in P2) = {prob_was_zero:.2f}\")\n",
    "                \n",
    "                samples = []\n",
    "                for _ in range(n_samples):\n",
    "                    if np.random.rand() < prob_was_zero:\n",
    "                        samples.append(0)\n",
    "                    else:\n",
    "                        samples.append(np.exp(np.random.normal(\n",
    "                            p1_params[1], p1_params[2]\n",
    "                        )))\n",
    "                samples = np.array(samples)\n",
    "            else:\n",
    "                # Non-zero in P2: use transition model\n",
    "                log_s2 = np.log(score_p2 + 1)\n",
    "                predicted_log_s1 = slope * log_s2 + intercept\n",
    "                \n",
    "                # Sample with uncertainty\n",
    "                log_s1_samples = np.random.normal(\n",
    "                    predicted_log_s1,\n",
    "                    residual_std * 1.5,  # Inflate for missing data\n",
    "                    n_samples\n",
    "                )\n",
    "                samples = np.exp(log_s1_samples) - 1\n",
    "                samples = np.clip(samples, 0, 500)  # Respect period 1 range\n",
    "            \n",
    "            results[node] = {\n",
    "                'period_2_score': score_p2,\n",
    "                'imputed_samples': samples,\n",
    "                'mean': samples.mean(),\n",
    "                'median': np.median(samples),\n",
    "                'ci_low': np.percentile(samples, 2.5),\n",
    "                'ci_high': np.percentile(samples, 97.5)\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run imputation\n",
    "results = bayesian_score_imputation(data)\n",
    "\n",
    "# Display results\n",
    "for node, result in results.items():\n",
    "    print(f\"\\nNode {node}:\")\n",
    "    print(f\"  Observed P2 score: {result['period_2_score']:.1f}\")\n",
    "    print(f\"  Imputed P1 score: {result['mean']:.1f} \"\n",
    "          f\"[{result['ci_low']:.1f}, {result['ci_high']:.1f}]\")\n",
    "\n",
    "# Convert to ranks (for each sample)\n",
    "def samples_to_rank_distribution(data, results, n_samples=5000):\n",
    "    \"\"\"Convert score samples to rank distributions\"\"\"\n",
    "    p1 = data[data['period'] == '2001-2005'].set_index('node')\n",
    "    \n",
    "    rank_samples = {node: [] for node in results.keys()}\n",
    "    \n",
    "    for sample_idx in range(n_samples):\n",
    "        # Get scores for this sample\n",
    "        scores = {}\n",
    "        for node in data['node'].unique():\n",
    "            if not pd.isna(p1.loc[node, 'score']):\n",
    "                scores[node] = p1.loc[node, 'score']\n",
    "            elif node in results:\n",
    "                scores[node] = results[node]['imputed_samples'][sample_idx]\n",
    "        \n",
    "        # Rank them (higher score = better rank)\n",
    "        sorted_nodes = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        ranks = {node: rank+1 for rank, (node, score) in enumerate(sorted_nodes)}\n",
    "        \n",
    "        for node in results.keys():\n",
    "            rank_samples[node].append(ranks[node])\n",
    "    \n",
    "    return rank_samples\n",
    "\n",
    "rank_distributions = samples_to_rank_distribution(data, results)\n",
    "\n",
    "for node, ranks in rank_distributions.items():\n",
    "    print(f\"\\nNode {node} - Rank distribution in period 1:\")\n",
    "    print(f\"  Mean rank: {np.mean(ranks):.1f}\")\n",
    "    print(f\"  Median rank: {np.median(ranks):.0f}\")\n",
    "    print(f\"  95% CI: [{np.percentile(ranks, 2.5):.0f}, \"\n",
    "          f\"{np.percentile(ranks, 97.5):.0f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0b205",
   "metadata": {},
   "source": [
    "-> This would work as a novelty measure perhaps (we would then add one paper at each and then compare the impact of it)? But for the analysis for the paper, I don't think this should be included? (The way we divided the periods makes it complicated to impute ; we can impute all the missing ones in one go, but then more assumptions need to be made?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619d6f3d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meta_wealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
